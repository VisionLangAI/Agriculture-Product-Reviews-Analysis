{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymb7Qv_H66wA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch pandas numpy scikit-learn matplotlib seaborn nltk wordcloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Ensure NLTK resources are available\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load Dataset\n",
        "df = pd.read_csv(\"product_reviews.csv\")  # Update with actual dataset path\n",
        "\n",
        "# Display dataset info\n",
        "print(\"\\nDataset Info:\\n\", df.info())\n",
        "print(\"\\nFirst Few Rows:\\n\", df.head())\n",
        "\n",
        "# -------------------------------\n",
        "# ** NLP PREPROCESSING **\n",
        "# -------------------------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and preprocesses text data.\"\"\"\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply NLP preprocessing to 'Description' column\n",
        "df[\"Cleaned_Description\"] = df[\"Description\"].astype(str).apply(preprocess_text)\n",
        "\n",
        "# -------------------------------\n",
        "# ** EXPLORATORY DATA ANALYSIS (EDA) **\n",
        "# -------------------------------\n",
        "# 1. Word Cloud for common words\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(df[\"Cleaned_Description\"]))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Common Words in Product Descriptions\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Sentiment Distribution Plot\n",
        "sns.countplot(x=df[\"Sentiment\"])\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Most Common Words\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", max_features=20)\n",
        "word_freq = vectorizer.fit_transform(df[\"Cleaned_Description\"])\n",
        "word_counts = dict(zip(vectorizer.get_feature_names_out(), np.ravel(word_freq.sum(axis=0))))\n",
        "sns.barplot(x=list(word_counts.keys()), y=list(word_counts.values()))\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 20 Most Common Words\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# ** SENTIMENT ANALYSIS (BERT & RoBERTa) **\n",
        "# -------------------------------\n",
        "df[\"Predicted_Sentiment\"] = \"\"\n",
        "\n",
        "models = {\n",
        "    \"BERT\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "    \"RoBERTa\": \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "}\n",
        "\n",
        "def analyze_sentiment(model_name, text):\n",
        "    \"\"\"Performs sentiment analysis using BERT/RoBERTa.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(models[model_name])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(models[model_name])\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    scores = softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "\n",
        "    labels = [\"Negative\", \"Neutral\", \"Positive\"]  # Adjust based on model\n",
        "    return labels[scores.argmax()]\n",
        "\n",
        "# Apply RoBERTa Sentiment Analysis\n",
        "for index, row in df.iterrows():\n",
        "    text = f\"{row['Product']} {row['Cleaned_Description']}\"\n",
        "    df.at[index, \"Predicted_Sentiment\"] = analyze_sentiment(\"RoBERTa\", text)\n",
        "\n",
        "# Save results\n",
        "df.to_csv(\"product_sentiment_results.csv\", index=False)\n",
        "print(\"Sentiment analysis completed and results saved!\")\n"
      ]
    }
  ]
}